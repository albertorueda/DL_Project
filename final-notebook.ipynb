{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e34a09d",
   "metadata": {},
   "source": [
    "# Final notebook DL project 29, group 113"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e3052d",
   "metadata": {},
   "source": [
    "Please run this cell first to have all of the correct libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ff19dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "from torch import dropout\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.nn import GRU, Module, Linear, ReLU, Sequential, Dropout\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from geopy.distance import geodesic\n",
    "from math import radians, sin, cos, sqrt, atan2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab19e182",
   "metadata": {},
   "source": [
    "## 1) Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b912fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_filter(g):\n",
    "        \"\"\"\n",
    "        Filter a DataFrame group representing a vessel track or segment.\n",
    "\n",
    "        Conditions:\n",
    "            - Length > MIN_TRACK_LENGTH\n",
    "            - Max SOG between MIN_SOG_FOR_TRACK and MAX_SOG_FOR_TRACK\n",
    "            - Timespan >= MIN_TRACK_TIMESPAN_SECONDS\n",
    "\n",
    "        Args:\n",
    "            g (pd.DataFrame): Vessel track data.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the track passes all filters.\n",
    "        \"\"\"\n",
    "        len_filt = len(g) > MIN_TRACK_LENGTH  # Min required length of track/segment\n",
    "        sog_filt = MIN_SOG_FOR_TRACK <= g[\"SOG\"].max() <= MAX_SOG_FOR_TRACK  # Remove stationary or outlier segments\n",
    "        time_filt = (g[\"Timestamp\"].max() - g[\"Timestamp\"].min()).total_seconds() >= MIN_TRACK_TIMESPAN_SECONDS\n",
    "        return len_filt and sog_filt and time_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2acd46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_linear(df_in, interval=f\"10min\", group_col=\"MMSI\", method=\"linear\",linear_cols = [\"Latitude\", \"Longitude\", \"SOG\"]):\n",
    "            \"\"\"\n",
    "            Interpolates Latitude, Longitude, and SOG linearly for each MMSI–Segment pair.\n",
    "\n",
    "            Args:\n",
    "                df_in (pd.DataFrame): Input data containing Latitude, Longitude, SOG, MMSI, Segment.\n",
    "                interval (str): Resampling interval (e.g. \"10min\").\n",
    "                group_col (str): Grouping column, usually \"MMSI\".\n",
    "                method (str): Interpolation method.\n",
    "\n",
    "            Returns:\n",
    "                pd.DataFrame: Interpolated data with [MMSI, Segment, Timestamp, Latitude, Longitude, SOG].\n",
    "            \"\"\"\n",
    "            df = df_in.copy()\n",
    "            df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "            df = df.dropna(subset=[group_col, \"Segment\"])\n",
    "            linear_cols = [\"Latitude\", \"Longitude\", \"SOG\"]\n",
    "            groups = []\n",
    "            # Group by both MMSI and Segment\n",
    "            for (mmsi, segment), g in df.groupby([group_col, \"Segment\"]):\n",
    "                g = g.sort_index()\n",
    "                g_numeric = g[linear_cols]\n",
    "                g2 = g_numeric.resample(interval).mean().interpolate(method=method)\n",
    "                g2[group_col] = mmsi\n",
    "                g2[\"Segment\"] = segment\n",
    "                groups.append(g2)\n",
    "            resampled_df = pd.concat(groups)\n",
    "            # Reset index and rename to Timestamp\n",
    "            resampled_df = resampled_df.reset_index().rename(columns={\"index\": \"Timestamp\"})\n",
    "            return resampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ef940ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_circular(df_in, interval=f\"10min\", group_col=\"MMSI\", circular_cols = [\"COG\"]):\n",
    "            \"\"\"\n",
    "            Interpolates angular features (e.g., COG) using sine/cosine trick to avoid discontinuities.\n",
    "\n",
    "            Args:\n",
    "                df_in (pd.DataFrame): Input data containing COG, MMSI, Segment.\n",
    "                interval (str): Resampling interval (e.g. \"10min\").\n",
    "                group_col (str): Grouping column, usually \"MMSI\".\n",
    "\n",
    "            Returns:\n",
    "                pd.DataFrame: Interpolated angular data with [MMSI, Segment, Timestamp, COG].\n",
    "            \"\"\"\n",
    "            df = df_in.copy()\n",
    "            df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "            df = df.dropna(subset=[group_col, \"Segment\"])\n",
    "            groups = []\n",
    "            for (mmsi, segment), g in df.groupby([group_col, \"Segment\"]):\n",
    "                g = g.sort_index()\n",
    "                # Resample on Timestamp\n",
    "                resampled = g[circular_cols].resample(interval).mean()\n",
    "                for col in circular_cols:\n",
    "                    # Forward fill to avoid NaNs before trig transform\n",
    "                    raw = resampled[col].copy().ffill()\n",
    "                    rad = np.deg2rad(raw)\n",
    "                    sin_interp = pd.Series(np.sin(rad), index=raw.index).interpolate(method=\"linear\")\n",
    "                    cos_interp = pd.Series(np.cos(rad), index=raw.index).interpolate(method=\"linear\")\n",
    "                    angle_rad = np.arctan2(sin_interp, cos_interp)\n",
    "                    resampled[col] = np.rad2deg(angle_rad) % 360\n",
    "                resampled[group_col] = mmsi\n",
    "                resampled[\"Segment\"] = segment\n",
    "                groups.append(resampled)\n",
    "            resampled_df = pd.concat(groups)\n",
    "            resampled_df = resampled_df.reset_index().rename(columns={\"index\": \"Timestamp\"})\n",
    "            return resampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e73969e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_create(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a cleaned DataFrame from raw AIS data.\n",
    "    \"\"\"\n",
    "\n",
    "    ### ================================================================\n",
    "    ### --- FLAGS ---\n",
    "    ### ================================================================\n",
    "    REMOVE_OUT_OF_BOUNDS = True\n",
    "    ENABLE_TYPE_FILTER = True\n",
    "    MMSI_VALIDATION = True\n",
    "    MMSI_MID_RANGE_VALIDATION = True\n",
    "    REMOVE_INVALID_TIMESTAMPS = True\n",
    "    REMOVE_DUPLICATES = True\n",
    "    REMOVE_ZERO_COORDS = True\n",
    "    REMOVE_SOG_COG_NAN_IF_NO_INTERPOLATION = True\n",
    "    REMOVE_HIGH_SOG = True\n",
    "    REMOVE_MIN_VALID_YEAR = 2015\n",
    "\n",
    "    ENABLE_TRACK_FILTER = True\n",
    "    ENABLE_SEGMENTATION = True\n",
    "\n",
    "    ENABLE_INTERPOLATION = True\n",
    "    ENABLE_NORMALIZATION = False\n",
    "\n",
    "    ENABLE_SORTING = True\n",
    "\n",
    "    ### ================================================================\n",
    "    ### --- CONSTANTS ---\n",
    "    ### ================================================================\n",
    "    KNOTS_TO_MS = 0.514444  # Conversion factor from knots to meters per second\n",
    "\n",
    "    BBOX = [60, 0, 50, 20]\n",
    "    VALID_TYPES = [\"Class A\", \"Class B\"]\n",
    "    MMSI_STANDARD_LENGTH = 9\n",
    "    VALID_MID_RANGE = (200, 775)\n",
    "    MAX_SOG_THRESHOLD = 30\n",
    "    MIN_VALID_YEAR = 2015\n",
    "    MIN_TRACK_LENGTH = 256\n",
    "    MIN_SOG_FOR_TRACK = 1\n",
    "    MAX_SOG_FOR_TRACK = 50\n",
    "\n",
    "    MIN_TRACK_TIMESPAN_SECONDS = 3600\n",
    "    SEGMENT_GAP_SECONDS = 900\n",
    "    MIN_SEGMENT_LENGTH = 10\n",
    "\n",
    "    ROUND_INTERVAL_MIN = 10\n",
    "\n",
    "    ### ================================================================\n",
    "    ### STEP 1: READ CSV\n",
    "    ### ================================================================\n",
    "    # Define expected data types to optimize memory usage and ensure consistency\n",
    "    dtypes = {\n",
    "        \"MMSI\": \"object\",\n",
    "        \"SOG\": float,\n",
    "        \"COG\": float,\n",
    "        \"Longitude\": float,\n",
    "        \"Latitude\": float,\n",
    "        \"# Timestamp\": \"object\",\n",
    "        \"Type of mobile\": \"object\",\n",
    "    }\n",
    "    usecols = list(dtypes.keys())\n",
    "    # Load raw AIS data from CSV file with specified columns and types\n",
    "    df = pd.read_csv(file_path, usecols=usecols, dtype=dtypes)\n",
    "    # Rename timestamp column immediately after reading\n",
    "    df = df.rename(columns={\"# Timestamp\": \"Timestamp\"})\n",
    "    initial_rows = len(df)\n",
    "    print(f\"[INFO] Loaded raw CSV: {initial_rows} rows.\")\n",
    "\n",
    "    ### --- CHECK FOR MISSING EXPECTED COLUMNS ---\n",
    "    # Validate presence of critical columns to avoid downstream errors\n",
    "    expected_columns = {\"MMSI\", \"Timestamp\", \"Latitude\", \"Longitude\", \"SOG\", \"COG\"}\n",
    "    missing_cols = expected_columns - set(df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"[WARNING] The following expected columns are missing from the dataframe: {missing_cols}\")\n",
    "\n",
    "    ### --- CONVERT SOG FROM KNOTS TO M/S IMMEDIATELY AFTER READING ---\n",
    "    # Convert SOG from knots to m/s for consistency in physical units\n",
    "    df[\"SOG\"] = KNOTS_TO_MS * df[\"SOG\"]  # All further processing will be done in meters per second\n",
    "\n",
    "    ### ================================================================\n",
    "    ### STEP 2: BASIC CLEANING\n",
    "    ### ================================================================\n",
    "    ### --- REMOVE OUT-OF-BOUNDS POSITIONS ---\n",
    "    # Filter out AIS points outside the defined geographic bounding box to focus on the area of interest\n",
    "    if REMOVE_OUT_OF_BOUNDS:\n",
    "        north, west, south, east = BBOX\n",
    "        initial_rows = len(df)\n",
    "        df = df[(df[\"Latitude\"] <= north) & (df[\"Latitude\"] >= south) & (df[\"Longitude\"] >= west) & (df[\"Longitude\"] <= east)]\n",
    "        removed = initial_rows - len(df)\n",
    "        print(f\"[INFO] Removed {removed} rows outside bounding box.\")\n",
    "\n",
    "    ### --- FILTER BY SHIP TYPE AND MMSI VALIDATION ---\n",
    "    # Keep only relevant ship types to reduce noise and ensure data quality\n",
    "    if ENABLE_TYPE_FILTER:\n",
    "        initial_rows = len(df)\n",
    "        df = df[df[\"Type of mobile\"].isin(VALID_TYPES)].drop(columns=[\"Type of mobile\"])\n",
    "        removed = initial_rows - len(df)\n",
    "        print(f\"[INFO] Removed {removed} rows not matching valid vessel types.\")\n",
    "\n",
    "    ### --- MMSI FORMAT VALIDATION ---\n",
    "    # Validate MMSI length to comply with standardized maritime identifiers\n",
    "    if MMSI_VALIDATION and MMSI_STANDARD_LENGTH:\n",
    "        initial_rows = len(df)\n",
    "        df = df[df[\"MMSI\"].str.len() == MMSI_STANDARD_LENGTH]  # Adhere to MMSI format\n",
    "        removed = initial_rows - len(df)\n",
    "        print(f\"[INFO] Removed {removed} rows failing MMSI length validation.\")\n",
    "    \n",
    "    ### --- MMSI MID RANGE VALIDATION ---\n",
    "    # Filter by MID range to ensure vessels are registered in expected regions\n",
    "    if MMSI_MID_RANGE_VALIDATION and VALID_MID_RANGE is not None:\n",
    "        initial_rows = len(df)\n",
    "        df = df[df[\"MMSI\"].str[:3].astype(int).between(VALID_MID_RANGE[0], VALID_MID_RANGE[1])]  # Adhere to MID standard\n",
    "        removed = initial_rows - len(df)\n",
    "        print(f\"[INFO] Removed {removed} rows failing MMSI MID range validation.\")\n",
    "\n",
    "    ### --- TIMESTAMP PARSING ---\n",
    "    # Convert to datetime for temporal analysis\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], format=\"%d/%m/%Y %H:%M:%S\", errors=\"coerce\")\n",
    "\n",
    "    ### --- TIMESTAMP VALIDATION ---\n",
    "    # Remove rows with invalid timestamps to maintain temporal integrity\n",
    "    if REMOVE_INVALID_TIMESTAMPS:\n",
    "        initial_rows = len(df)\n",
    "        invalid_ts = df[\"Timestamp\"].isna().sum()\n",
    "        if invalid_ts > 0:\n",
    "            print(f\"[WARNING] {invalid_ts} / {initial_rows} rows have invalid timestamps and will be removed.\")\n",
    "        df = df.dropna(subset=[\"Timestamp\"])\n",
    "        removed = initial_rows - len(df)\n",
    "        print(f\"[INFO] Removed {removed} rows with invalid timestamps.\")\n",
    "\n",
    "    ### --- REMOVE DUPLICATE ENTRIES ---\n",
    "    # Remove duplicate AIS messages to avoid bias and redundancy\n",
    "    if REMOVE_DUPLICATES:\n",
    "        initial_rows = len(df)\n",
    "        df = df.drop_duplicates([\"Timestamp\", \"MMSI\", ], keep=\"first\")\n",
    "        removed = initial_rows - len(df)\n",
    "        print(f\"[INFO] Removed {removed} duplicate rows.\")\n",
    "\n",
    "    ### --- REMOVE INVALID COORDS ---\n",
    "    # Exclude points with zero coordinates which are likely erroneous\n",
    "    if REMOVE_ZERO_COORDS:\n",
    "        initial_rows = len(df)\n",
    "        df = df[(df[\"Latitude\"] != 0) & (df[\"Longitude\"] != 0)]\n",
    "        removed = initial_rows - len(df)\n",
    "        print(f\"[INFO] Removed {removed} rows with zero coordinates.\")\n",
    "\n",
    "    ### --- REMOVE NaN SOG/COG IF NO INTERPOLATION ---\n",
    "    # Drop rows with missing speed or course if interpolation is not enabled to keep data consistent\n",
    "    if REMOVE_SOG_COG_NAN_IF_NO_INTERPOLATION and not ENABLE_INTERPOLATION:\n",
    "        initial_rows = len(df)\n",
    "        df = df.dropna(subset=[\"SOG\", \"COG\"])\n",
    "        removed = initial_rows - len(df)\n",
    "        print(f\"[INFO] Removed {removed} rows with NaN SOG/COG (no interpolation).\")\n",
    "\n",
    "    ### --- REMOVE HIGH SOG VALUES ---\n",
    "    # Filter out unrealistic high speeds to remove noise and outliers\n",
    "    if REMOVE_HIGH_SOG and MAX_SOG_THRESHOLD is not None:\n",
    "        initial_rows = len(df)\n",
    "        df = df[df[\"SOG\"] < MAX_SOG_THRESHOLD]\n",
    "        removed = initial_rows - len(df)\n",
    "        print(f\"[INFO] Removed {removed} rows with SOG >= {MAX_SOG_THRESHOLD} m/s.\")\n",
    "\n",
    "    ### --- REMOVE DATA BEFORE MIN VALID YEAR ---\n",
    "    # Focus analysis on recent data by excluding older records\n",
    "    if REMOVE_MIN_VALID_YEAR and MIN_VALID_YEAR is not None:\n",
    "        initial_rows = len(df)\n",
    "        df = df[df[\"Timestamp\"].dt.year >= MIN_VALID_YEAR]\n",
    "        removed = initial_rows - len(df)\n",
    "        print(f\"[INFO] Removed {removed} rows before year {MIN_VALID_YEAR}.\")\n",
    "\n",
    "    print(f\"[INFO] Dataframe after basic cleaning has {len(df)} rows and {df['MMSI'].nunique()} unique MMSIs.\")\n",
    "\n",
    "    ### ================================================================\n",
    "    ### STEP 3: TRACK FILTERING AND SEGMENTATION\n",
    "    ### ================================================================\n",
    "    # Filter out tracks that do not meet minimum criteria to improve model training quality\n",
    "    if ENABLE_TRACK_FILTER:\n",
    "        initial_rows = len(df)\n",
    "        df = df.groupby(\"MMSI\").filter(track_filter)\n",
    "        removed = initial_rows - len(df)\n",
    "        print(f\"[INFO] Removed {removed} rows by track filtering.\")\n",
    "\n",
    "    if ENABLE_SEGMENTATION:\n",
    "        # Segment tracks based on time gaps to isolate continuous vessel movements\n",
    "        df['Segment'] = df.groupby('MMSI')['Timestamp'].transform(\n",
    "            lambda x: (x.diff().dt.total_seconds().fillna(0) >= SEGMENT_GAP_SECONDS).cumsum()\n",
    "        )\n",
    "        initial_rows = len(df)\n",
    "        df = df.groupby([\"MMSI\", \"Segment\"]).filter(track_filter)\n",
    "        removed = initial_rows - len(df)\n",
    "        print(f\"[INFO] Removed {removed} rows by segment track filtering.\")\n",
    "\n",
    "        initial_rows = len(df)\n",
    "        # Remove short segments after segmentation to keep only substantial trajectories\n",
    "        df = df.groupby([\"MMSI\", \"Segment\"]).filter(lambda g: len(g) >= MIN_SEGMENT_LENGTH)\n",
    "        removed = initial_rows - len(df)\n",
    "        print(f\"[INFO] Removed {removed} rows from segments shorter than {MIN_SEGMENT_LENGTH} rows.\")\n",
    "\n",
    "        df = df.reset_index(drop=True)\n",
    "        # Store MMSI–Timestamp–Segment mapping before interpolation (so we can merge it back later)\n",
    "        segment_df = df[[\"MMSI\", \"Timestamp\", \"Segment\"]].copy()\n",
    "    else:\n",
    "        df['Segment'] = 0  # Default segment assignment when segmentation is disabled\n",
    "\n",
    "    print(f\"[INFO] Dataframe after track filtering and segmentation has {len(df)} rows and {df['MMSI'].nunique()} unique MMSIs.\")\n",
    "\n",
    "    ### ================================================================\n",
    "    ### STEP 4: INTERPOLATION AND NORMALIZATION\n",
    "    ### ================================================================\n",
    "    if ENABLE_INTERPOLATION:\n",
    "        print(f\"[INFO] Interpolating dataset at {ROUND_INTERVAL_MIN}-minute intervals using pandas.resample...\")\n",
    "        # Set timestamp as index for time-based resampling\n",
    "        df = df.set_index(\"Timestamp\")\n",
    "\n",
    "        # ================================================================\n",
    "        # --- LINEAR INTERPOLATION FOR LINEAR VARIABLES ---\n",
    "        # ================================================================\n",
    "        # Columns that can be interpolated linearly\n",
    "        linear_cols = [\"Latitude\", \"Longitude\", \"SOG\"]\n",
    "        \n",
    "        # Perform linear interpolation for position and speed, including Segment\n",
    "        df_linear = interpolate_linear(\n",
    "            df[[\"MMSI\", \"Segment\"] + linear_cols],\n",
    "            interval=f\"{ROUND_INTERVAL_MIN}min\",\n",
    "            group_col=\"MMSI\",\n",
    "            method=\"linear\"\n",
    "        )\n",
    "        print(f\"[INFO] Linear interpolated rows: {len(df_linear)}\")\n",
    "\n",
    "        # ================================================================\n",
    "        # --- CIRCULAR INTERPOLATION FOR ANGULAR VARIABLES ---\n",
    "        # ================================================================\n",
    "        # Columns that represent angles and require special interpolation to handle wrap-around\n",
    "        circular_cols = [\"COG\"]  # Columns measured in degrees on a circular domain\n",
    "\n",
    "        # Apply circular interpolation per MMSI–Segment to handle angular data correctly\n",
    "        df_cog = interpolate_circular(\n",
    "            df[[\"MMSI\", \"Segment\"] + circular_cols],\n",
    "            interval=f\"{ROUND_INTERVAL_MIN}min\",\n",
    "            group_col=\"MMSI\"\n",
    "        )\n",
    "        print(f\"[INFO] Circular interpolated rows: {len(df_cog)}\")\n",
    "\n",
    "        # ================================================================\n",
    "        # --- COMBINE INTERPOLATIONS ---\n",
    "        # ================================================================\n",
    "        # Merge linear and circular interpolated data on MMSI + Timestamp\n",
    "        # Ensures both interpolation types align in time after resampling.\n",
    "        df_interp = df_linear.merge(\n",
    "            df_cog,\n",
    "            on=[\"MMSI\", \"Segment\", \"Timestamp\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        # Segment column is already present in both; no need to merge segment_df again\n",
    "        df = df_interp\n",
    "        # Drop any remaining rows with missing critical values\n",
    "        initial_rows = len(df)\n",
    "        df = df.dropna(subset=[\"Latitude\", \"Longitude\", \"SOG\", \"COG\"])\n",
    "        removed = initial_rows - len(df)\n",
    "        print(f\"[INFO] Interpolated dataset now has {len(df)} rows (removed {removed} rows with missing critical values).\")\n",
    "\n",
    "    if ENABLE_NORMALIZATION:\n",
    "        # Normalize Latitude and Longitude to [0,1] to facilitate machine learning model convergence\n",
    "        print(\"[INFO] Normalizing Latitude and Longitude columns to [0,1] range.\")\n",
    "        df.loc[:, \"Latitude\"] = (df[\"Latitude\"] - df[\"Latitude\"].min()) / (df[\"Latitude\"].max() - df[\"Latitude\"].min())\n",
    "        df.loc[:, \"Longitude\"] = (df[\"Longitude\"] - df[\"Longitude\"].min()) / (df[\"Longitude\"].max() - df[\"Longitude\"].min())\n",
    "\n",
    "    # Sort data by MMSI and Timestamp to maintain chronological order for each vessel\n",
    "    if ENABLE_SORTING:\n",
    "        df = df.sort_values([\"MMSI\", \"Segment\", \"Timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "    ### ================================================================\n",
    "    ### STEP 4: SUMMARY AND FINALIZATION\n",
    "    ### ================================================================\n",
    "    # Print summary statistics about the cleaned dataset\n",
    "    print(f\"[INFO] Final dataset stats:\")\n",
    "    print(f\"  - Total rows: {len(df)}\")\n",
    "    print(f\"  - Unique MMSIs: {df['MMSI'].nunique()}\")\n",
    "    print(f\"  - Time span: {df['Timestamp'].min()} to {df['Timestamp'].max()}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fcca2997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df: pd.DataFrame, train_frac: float, val_frac: float) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Splits a cleaned AIS DataFrame into train/val/test splits based on MMSI-level vessel grouping.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Cleaned AIS dataset.\n",
    "        train_frac (float): Fraction of vessels to include in training set.\n",
    "        val_frac (float): Fraction of vessels to include in validation set.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: DataFrames for train, validation, and test sets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine number of unique vessels to split by vessel rather than by row\n",
    "    ship_number = df[\"MMSI\"].nunique()\n",
    "\n",
    "    # Randomly sample vessels for training set\n",
    "    train_ships = random.sample(list(df[\"MMSI\"].unique()), int(ship_number * train_frac))\n",
    "    # Sample vessels for validation set excluding training vessels\n",
    "    val_ships = random.sample(list(set(df[\"MMSI\"].unique()) - set(train_ships)), int(ship_number * val_frac))\n",
    "    # Remaining vessels assigned to test set\n",
    "    test_ships = list(set(df[\"MMSI\"].unique()) - set(train_ships) - set(val_ships))\n",
    "\n",
    "    # Create DataFrames for each split based on vessel membership\n",
    "    df_train = df[df[\"MMSI\"].isin(train_ships)].copy().reset_index(drop=True)\n",
    "    df_val = df[df[\"MMSI\"].isin(val_ships)].copy().reset_index(drop=True)\n",
    "    df_test = df[df[\"MMSI\"].isin(test_ships)].copy().reset_index(drop=True)\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50058db",
   "metadata": {},
   "source": [
    "We have already cleaned and split the raw files, so the next cell will not work, but this is what gave us the final csv we worked with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1500291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning raw data folder for AIS CSV files...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No CSV files found in raw_data/",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m raw_files = glob.glob(\u001b[33m\"\u001b[39m\u001b[33mraw data/*.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_files:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo CSV files found in raw_data/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m raw_files:\n\u001b[32m      8\u001b[39m     base = os.path.basename(file_path)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No CSV files found in raw_data/"
     ]
    }
   ],
   "source": [
    "print(\"Scanning raw data folder for AIS CSV files...\")\n",
    "raw_files = glob.glob(\"raw data/*.csv\")\n",
    "\n",
    "if not raw_files:\n",
    "    raise FileNotFoundError(\"No CSV files found in raw_data/\")\n",
    "\n",
    "for file_path in raw_files:\n",
    "    base = os.path.basename(file_path)\n",
    "    name, _ = os.path.splitext(base)\n",
    "    print(f\"\\n=== Processing file: {base} ===\")\n",
    "\n",
    "    # --- Preprocess file ---\n",
    "    df = df_create(file_path)\n",
    "\n",
    "    # --- Split dataset ---\n",
    "    print(\"Splitting into train/val/test...\")\n",
    "    train_df, val_df, test_df = split_dataset(df, train_frac=0.7, val_frac=0.15)\n",
    "\n",
    "    # --- Export CSVs ---\n",
    "    os.makedirs(\"datasplits\", exist_ok=True)  # Ensure datasplits directory exists\n",
    "    train_df.to_csv(f\"datasplits/train/train_{name}.csv\", index=False)\n",
    "    val_df.to_csv(f\"datasplits/val/val_{name}.csv\", index=False)\n",
    "    test_df.to_csv(f\"datasplits/test/test_{name}.csv\", index=False)\n",
    "\n",
    "    print(f\"Saved: train_{name}.csv, val_{name}.csv, test_{name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20888ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AISDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for AIS (Automatic Identification System) trajectory data.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str or pd.DataFrame): Path to CSV or DataFrame containing AIS data.\n",
    "        seq_input_length (int): Number of time steps in the input sequence.\n",
    "        seq_output_length (int): Number of time steps in the output sequence.\n",
    "        stats (tuple, optional): Tuple of (lat_min, lat_max, lon_min, lon_max, sog_max) to apply consistent normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_path: str, seq_input_length: int = 5, seq_output_length: int = 5, stats=None):\n",
    "        # We only support seq_output_length of equal to seq_input_length (same window size input-output) \n",
    "        if (seq_input_length != seq_output_length):\n",
    "            raise ValueError(\"Seq_input_length must be equal to seq_output_length\")\n",
    "        \n",
    "        self.dataframe = dataset_path if isinstance(dataset_path, pd.DataFrame) else pd.read_csv(dataset_path)\n",
    "\n",
    "        if 'Timestamp' in self.dataframe.columns:\n",
    "            self.dataframe.loc[:, 'Timestamp'] = pd.to_datetime(self.dataframe.loc[:, 'Timestamp'])\n",
    "            self.dataframe = self.dataframe.sort_values(by=['MMSI', 'Timestamp']).reset_index(drop=True)\n",
    "        \n",
    "        self.seq_input_length = seq_input_length\n",
    "        self.seq_output_length = seq_output_length\n",
    "        self.valid_idxs = []\n",
    "        \n",
    "\n",
    "        # --- 1. INDEXING LOGIC ---\n",
    "        # Precompute valid indices\n",
    "        unique_mmsis = self.dataframe.loc[:, 'MMSI'].unique()\n",
    "        for mmsi in unique_mmsis:\n",
    "            mmsi_mask = self.dataframe.loc[:, 'MMSI'] == mmsi\n",
    "            mmsi_df = self.dataframe[mmsi_mask]\n",
    "            \n",
    "            # Iterate through segments\n",
    "            for segment_id, segment_data in mmsi_df.groupby('Segment'):\n",
    "                if len(segment_data) < (self.seq_input_length + self.seq_output_length):\n",
    "                   continue\n",
    "                indices = segment_data.index\n",
    "                num_sequences = len(segment_data) - (self.seq_input_length + self.seq_output_length) + 1\n",
    "                for i in range(num_sequences):\n",
    "                    self.valid_idxs.append(indices[i])\n",
    "\n",
    "        # --- 2. NORMALIZATION LOGIC ---\n",
    "        # If no stats are provided, compute from current dataset\n",
    "        if stats is None:\n",
    "            self.lat_min = self.dataframe.loc[:, 'Latitude'].min()\n",
    "            self.lat_max = self.dataframe.loc[:, 'Latitude'].max()\n",
    "            self.lon_min = self.dataframe.loc[:, 'Longitude'].min()\n",
    "            self.lon_max = self.dataframe.loc[:, 'Longitude'].max()\n",
    "            self.sog_max = self.dataframe.loc[:, 'SOG'].max() # Add speed max\n",
    "        else:\n",
    "            self.lat_min, self.lat_max, self.lon_min, self.lon_max, self.sog_max = stats\n",
    "\n",
    "        # Save stats so we can retrieve them later\n",
    "        self.stats = (self.lat_min, self.lat_max, self.lon_min, self.lon_max, self.sog_max)\n",
    "\n",
    "        # Apply Normalization\n",
    "        # epsilon 1e-6 to avoid division by zero\n",
    "        self.dataframe.loc[:, 'Latitude'] = (self.dataframe.loc[:, 'Latitude'] - self.lat_min) / (self.lat_max - self.lat_min + 1e-6)\n",
    "        self.dataframe.loc[:, 'Longitude'] = (self.dataframe.loc[:, 'Longitude'] - self.lon_min) / (self.lon_max - self.lon_min + 1e-6)\n",
    "        \n",
    "        # Normalize Speed (SOG) roughly 0-1\n",
    "        self.dataframe.loc[:, 'SOG'] = self.dataframe.loc[:, 'SOG'] / (self.sog_max + 1e-6)           \n",
    "                \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: Number of valid input-output sequences.\n",
    "        \"\"\"\n",
    "        return len(self.valid_idxs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Constructs one input-output pair from the AIS data.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sequence.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Tuple of input tensor x and target tensor y.\n",
    "        \"\"\"\n",
    "        real_idx = self.valid_idxs[idx]\n",
    "\n",
    "        # --- 3. COG (Degrees to Radians) ---\n",
    "        cog_deg = self.dataframe.iloc[real_idx: real_idx + self.seq_input_length]['COG'].to_numpy(dtype=float)\n",
    "        cog_rad = np.deg2rad(cog_deg) # Convert to radians\n",
    "        cog_tensor = torch.tensor(cog_rad, dtype=torch.float32)\n",
    "        \n",
    "        cog_sin = torch.sin(cog_tensor)\n",
    "        cog_cos = torch.cos(cog_tensor)\n",
    "\n",
    "        # x features\n",
    "        x_data = self.dataframe.iloc[real_idx: real_idx + self.seq_input_length][['Latitude', 'Longitude', 'SOG']].to_numpy(dtype=float)\n",
    "        x_tensor = torch.tensor(x_data, dtype=torch.float32)\n",
    "        \n",
    "        # Concatenate: Lat, Lon, SOG, Sin, Cos\n",
    "        x = torch.cat((x_tensor, cog_sin.unsqueeze(-1), cog_cos.unsqueeze(-1)), dim=-1)\n",
    "\n",
    "        # y labels\n",
    "        y_data = self.dataframe.iloc[real_idx + self.seq_input_length: \n",
    "                                     real_idx + self.seq_input_length + self.seq_output_length][['Latitude', 'Longitude']].to_numpy(dtype=float)\n",
    "        y = torch.tensor(y_data, dtype=torch.float32)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fbf056",
   "metadata": {},
   "source": [
    "## 2) Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec4c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    '''\n",
    "    A GRU-based sequence-to-sequence model for predicting future positions based on input sequences.\n",
    "    Args:\n",
    "        input_size (int): The number of expected features in the input (default is 5).\n",
    "        embed_size (int): The size of the embedding layer (default is 64).\n",
    "        hidden_size (int): The number of features in the hidden state of the GRU (default is 256).\n",
    "        output_size (int): The number of expected features in the output (default is 2).\n",
    "        num_layers (int): Number of hidden layers in the GRU (default is 1).\n",
    "        dropout (float): Dropout probability for the GRU layers (default is 0.1).\n",
    "        first_linear (bool): Whether to include a linear embedding layer before the GRU (default is True).\n",
    "    Returns:\n",
    "        torch.Tensor: The output tensor containing predicted positions with shape (batch_size, seq_length, output_size).\n",
    "    '''\n",
    "    def __init__(self, input_size = 5, embed_size = 64, hidden_size = 256, output_size = 2, num_layers=1, dropout=0.1, first_linear=True):\n",
    "        super().__init__()\n",
    "        self.first_linear = first_linear\n",
    "\n",
    "        if first_linear:\n",
    "            # Map input from size 5 (input_size) to size 64 (embed_size)\n",
    "            self.embedding = nn.Linear(input_size, embed_size)\n",
    "            # GRU works in 64-dim space \n",
    "            self.gru = nn.GRU(\n",
    "                embed_size,\n",
    "                hidden_size,\n",
    "                num_layers,\n",
    "                bias=True,\n",
    "                batch_first=True,\n",
    "                dropout = dropout ,\n",
    "                bidirectional=False\n",
    "            )\n",
    "        else:\n",
    "            # GRU works in 4-dim space \n",
    "            self.gru = nn.GRU(\n",
    "                input_size,\n",
    "                hidden_size,\n",
    "                num_layers,\n",
    "                bias=True,\n",
    "                batch_first=True,\n",
    "                dropout = dropout ,\n",
    "                bidirectional=False\n",
    "            )\n",
    "\n",
    "        # Map GRU outputs from size 64 (embed_size) to size 2 (input/output_size) again \n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.first_linear:\n",
    "            x = self.embedding(x)        # (batch, seq, 64)\n",
    "        out, hidden = self.gru(x)    # (batch, seq, 64)\n",
    "        out = self.fc_out(out)       # (batch, seq, 2)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae852e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    '''\n",
    "    A LSTM-based sequence-to-sequence model for predicting future positions based on input sequences.\n",
    "    Args:\n",
    "        input_size (int): The number of expected features in the input (default is 5).\n",
    "        embed_size (int): The size of the embedding layer (default is 64).\n",
    "        hidden_size (int): The number of features in the hidden state of the LSTM (default is 256).\n",
    "        output_size (int): The number of expected features in the output (default is 2).\n",
    "        num_layers (int): Number of hidden layers in the LSTM (default is 1).\n",
    "        dropout (float): Dropout probability for the LSTM layers (default is 0.0).\n",
    "        first_linear (bool): Whether to include a linear embedding layer before the LSTM (default is True).\n",
    "    Returns:\n",
    "        torch.Tensor: The output tensor containing predicted positions with shape (batch_size, seq_length, output_size).\n",
    "\n",
    "    '''\n",
    "    def __init__(self, input_size = 5, embed_size = 64, hidden_size = 256, output_size = 2, num_layers=1, dropout=0.0, first_linear=True):\n",
    "        super().__init__()\n",
    "        self.first_linear = first_linear\n",
    "\n",
    "        if self.first_linear:\n",
    "            # Map input from size 5 (input_size) to size 64 (embed_size)\n",
    "            self.embedding = nn.Linear(input_size, embed_size)\n",
    "\n",
    "            # LSTM works in 64-dim space \n",
    "            self.lstm = nn.LSTM(\n",
    "                embed_size,\n",
    "                hidden_size,\n",
    "                num_layers,\n",
    "                bias=True,\n",
    "                batch_first=True,\n",
    "                dropout = dropout ,\n",
    "                bidirectional=False\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            # LSTM works in 4-dim space \n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size,\n",
    "                hidden_size,\n",
    "                num_layers,\n",
    "                bias=True,\n",
    "                batch_first=True,\n",
    "                dropout = dropout ,\n",
    "                bidirectional=False\n",
    "            )\n",
    "\n",
    "        # Map LSTM outputs from size 64 (embed_size) to size 2 (input/output_size) again \n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.first_linear:\n",
    "            x = self.embedding(x)        # (batch, seq, 64)\n",
    "        out, (hidden, cell) = self.lstm(x)    # (batch, seq, 64)\n",
    "        out = self.fc_out(out)       # (batch, seq, 2)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebce1f78",
   "metadata": {},
   "source": [
    "## 3) Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85896214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HaversineLoss(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 lat_min, lat_max,\n",
    "                 lon_min, lon_max,\n",
    "                 radius_earth_km=6371.0):\n",
    "        super().__init__()\n",
    "        self.lat_min = lat_min\n",
    "        self.lat_max = lat_max\n",
    "        self.lon_min = lon_min\n",
    "        self.lon_max = lon_max\n",
    "        self.radius_earth_km = radius_earth_km\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_pred: (batch, seq, 2) lat/lon normalizados [0,1]\n",
    "            y_true: (batch, seq, 2) lat/lon normalizados [0,1]\n",
    "        \"\"\"\n",
    "\n",
    "        # --- DESNORMALIZAR ---\n",
    "        lat_true = y_true[..., 0] * (self.lat_max - self.lat_min) + self.lat_min\n",
    "        lon_true = y_true[..., 1] * (self.lon_max - self.lon_min) + self.lon_min\n",
    "\n",
    "        lat_pred = y_pred[..., 0] * (self.lat_max - self.lat_min) + self.lat_min\n",
    "        lon_pred = y_pred[..., 1] * (self.lon_max - self.lon_min) + self.lon_min\n",
    "\n",
    "        # --- PASAR A RADIANES ---\n",
    "        lat1 = torch.deg2rad(lat_true)\n",
    "        lon1 = torch.deg2rad(lon_true)\n",
    "        lat2 = torch.deg2rad(lat_pred)\n",
    "        lon2 = torch.deg2rad(lon_pred)\n",
    "\n",
    "        # --- HAVERSINE ---\n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "\n",
    "        a = (torch.sin(dlat / 2) ** 2 +\n",
    "             torch.cos(lat1) * torch.cos(lat2) *\n",
    "             torch.sin(dlon / 2) ** 2)\n",
    "\n",
    "        a = torch.clamp(a, 0.0, 1.0)\n",
    "        eps = 1e-9\n",
    "\n",
    "        c = 2 * torch.atan2(torch.sqrt(a + eps), torch.sqrt(1 - a + eps))\n",
    "        distance = self.radius_earth_km * c  # => km\n",
    "\n",
    "        return torch.mean(distance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2bed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridTrajectoryLoss(torch.nn.Module):\n",
    "    def __init__(self, w_pos=1.0, w_ang=0.2, w_spd=0.1, radius_km=6371.0, max_pos_error_km=10.0):\n",
    "        super().__init__()\n",
    "        self.w_pos = w_pos\n",
    "        self.w_ang = w_ang\n",
    "        self.w_spd = w_spd\n",
    "        self.radius_km = radius_km\n",
    "        self.max_pos_error_km = max_pos_error_km\n",
    "\n",
    "    def haversine_dist(self, p1, p2):\n",
    "        \"\"\"\n",
    "        Calculates Great Circle distance (km) between points.\n",
    "        Input: (Batch, ..., 2) where 0=Lat, 1=Lon in RADIANS.\n",
    "        \"\"\"\n",
    "        lat1, lon1 = p1[..., 0], p1[..., 1]\n",
    "        lat2, lon2 = p2[..., 0], p2[..., 1]\n",
    "\n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "\n",
    "        a = (torch.sin(dlat / 2) ** 2 +\n",
    "             torch.cos(lat1) * torch.cos(lat2) * torch.sin(dlon / 2) ** 2)\n",
    "        \n",
    "        # Stability clamp\n",
    "        a = torch.clamp(a, 0.0, 1.0)\n",
    "        \n",
    "        c = 2 * torch.atan2(torch.sqrt(a + 1e-9), torch.sqrt(1 - a + 1e-9))\n",
    "        return self.radius_km * c\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_pred: (Batch, Seq_Len, 2) -> Lat/Lon in RADIANS\n",
    "            y_true: (Batch, Seq_Len, 2) -> Lat/Lon in RADIANS\n",
    "        \"\"\"\n",
    "\n",
    "        # --- 1. Position Loss (Haversine -> Tanh) ---\n",
    "        dist_km = self.haversine_dist(y_pred, y_true)\n",
    "        \n",
    "        # Normalize: 0 to 1\n",
    "        # If the error is > max_pos_error_km, the loss saturates to 1.0\n",
    "        loss_pos = torch.tanh(dist_km.mean() / self.max_pos_error_km)\n",
    "\n",
    "\n",
    "        # --- 2. Angle Loss (Euclidean Vector Approximation) ---\n",
    "        vec_pred = y_pred[:, -1, :] - y_pred[:, 0, :]\n",
    "        vec_true = y_true[:, -1, :] - y_true[:, 0, :]\n",
    "        \n",
    "        # Calculate Cosine Similarity\n",
    "        cos_sim = F.cosine_similarity(vec_pred, vec_true, dim=1, eps=1e-8)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        # Range of cosine is [-1, 1], so (1 - cos) is [0, 2]. Divide by 2.\n",
    "        loss_ang = (1.0 - cos_sim).mean() / 2.0\n",
    "\n",
    "\n",
    "        # --- 3. Speed Loss (Euclidean Diff -> Tanh) ---\n",
    "        vel_pred = torch.diff(y_pred, dim=1)\n",
    "        vel_true = torch.diff(y_true, dim=1)\n",
    "        \n",
    "        # Magnitude of the difference vector\n",
    "        speed_pred = torch.linalg.norm(vel_pred, dim=2)\n",
    "        speed_true = torch.linalg.norm(vel_true, dim=2)\n",
    "        \n",
    "        # Squared Error\n",
    "        speed_sq_err = (speed_pred - speed_true) ** 2\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        loss_spd = torch.tanh(speed_sq_err.mean())\n",
    "\n",
    "\n",
    "        # --- Combine ---\n",
    "        total_loss = (self.w_pos * loss_pos) + \\\n",
    "                     (self.w_ang * loss_ang) + \\\n",
    "                     (self.w_spd * loss_spd)\n",
    "\n",
    "        return total_loss, {\n",
    "            \"loss_pos\": loss_pos.item(),\n",
    "            \"loss_ang\": loss_ang.item(),\n",
    "            \"loss_spd\": loss_spd.item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb5a645",
   "metadata": {},
   "source": [
    "## 4) Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d40950",
   "metadata": {},
   "source": [
    "We trained the model in 4 different steps :\n",
    "<br> <br>\n",
    "1) Training the parameters for GRU & LSTM and Haversine & MAE loss: **number of hidden units per layer** and **number of hidden layers** <br>\n",
    "2) Finetuning for the best above mentioned model : **Batch size**, **Dropout rate** and **Window size** <br>\n",
    "3) Training the above mentioned best model <br>\n",
    "4) Training this same model but with more data (4 days) <br>\n",
    "<br>\n",
    "In the cell below, we have kept step 3): training with 1 day of data the best model. This cell should run, but could take a lot of time, so we recommend to run this on a server with GPUs. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c202e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 2769\n",
      "Number of validation batches: 554\n",
      "Using device: cpu\n",
      "\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.106579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     77\u001b[39m val_bar = tqdm(val_loader, desc=\u001b[33m\"\u001b[39m\u001b[33mValidation\u001b[39m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_bar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36mAISDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     99\u001b[39m x = torch.cat((x_tensor, cog_sin.unsqueeze(-\u001b[32m1\u001b[39m), cog_cos.unsqueeze(-\u001b[32m1\u001b[39m)), dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# y labels\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m y_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreal_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_input_length\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mreal_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_input_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_output_length\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLatitude\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLongitude\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.to_numpy(dtype=\u001b[38;5;28mfloat\u001b[39m)\n\u001b[32m    104\u001b[39m y = torch.tensor(y_data, dtype=torch.float32)\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/base.py:6245\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6243\u001b[39m \u001b[38;5;66;03m# Count missing values\u001b[39;00m\n\u001b[32m   6244\u001b[39m missing_mask = indexer < \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m6245\u001b[39m nmissing = \u001b[43mmissing_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/numpy/_core/_methods.py:52\u001b[39m, in \u001b[36m_sum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     51\u001b[39m          initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "sequence_input_length = 3\n",
    "sequence_output_length = 3\n",
    "batch_size = 32 \n",
    "dropout_num = 0.1 #FOR THE DROPOUT LAYER IN THE MODEL\n",
    "lr = 0.00001 #LEARNING RATE FOR ADAM OPTIMIZER\n",
    "num_epochs = 1000 #NUMBER OF EPOCHS TO TRAIN\n",
    "patience = 5 #EARLY STOPPING PATIENCE\n",
    "\n",
    "type_of_loss = 'MAE'\n",
    "trainset = AISDataset('datasplits/train/train_aisdk-2025-02-27.csv', seq_input_length=sequence_input_length, seq_output_length=sequence_output_length)\n",
    "\n",
    "# 2. Extract stats from Train Set\n",
    "train_stats = trainset.stats\n",
    "# 3. Pass stats to Validation Set\n",
    "valset = AISDataset('datasplits/val/val_aisdk-2025-02-27.csv', seq_input_length=sequence_input_length, seq_output_length=sequence_output_length, stats=train_stats)\n",
    "    \n",
    "# Create data loaders\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "lat_min = min(trainset.lat_min, valset.lat_min)\n",
    "lat_max = max(trainset.lat_max, valset.lat_max)\n",
    "lon_min = min(trainset.lon_min, valset.lon_min)\n",
    "lon_max = max(trainset.lon_max, valset.lon_max)\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Initialize the model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "training_losses = {}\n",
    "validation_losses = {}\n",
    "early_stopping_epochs = {}\n",
    "val_losses_i = {}\n",
    "train_losses_i = {}    \n",
    "\n",
    "model = GRUModel(input_size=5, embed_size=64, hidden_size=256, output_size=2, num_layers=2, dropout=0.1).to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "if type_of_loss == 'HAVERSINE': \n",
    "    loss_fn = HaversineLoss(lat_min, lat_max, lon_min, lon_max)\n",
    "elif type_of_loss == 'MAE':\n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "train_losses_list = []\n",
    "val_losses_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Training step with tqdm over batches\n",
    "    batch_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for batch_idx, (data, target) in enumerate(batch_bar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        l = loss_fn(output, target)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += l.item()\n",
    "        batch_bar.set_postfix(loss=l.item())\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses_list.append(train_loss)\n",
    "    print(f\"Training Loss: {train_loss:.6f}\")\n",
    "        \n",
    "    # Validation step with tqdm\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_bar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_bar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            batch_loss = loss_fn(output, target).item()\n",
    "            val_loss += batch_loss\n",
    "            val_bar.set_postfix(loss=batch_loss)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses_list.append(val_loss)\n",
    "    print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "        \n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        train_loss_model = train_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "            break\n",
    "    \n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'results/models/mae_final_model.pth')\n",
    "    \n",
    "# Save training and validation losses\n",
    "training_losses['final_model'] = train_losses_list\n",
    "validation_losses['final_model'] = val_losses_list\n",
    "early_stopping_epochs['final_model'] = epoch + 1\n",
    "    \n",
    "with open('results/results_final_model_mae.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'training_losses': training_losses,\n",
    "        'validation_losses': validation_losses,\n",
    "        'early_stopping_epochs': early_stopping_epochs\n",
    "    }, f)\n",
    "    \n",
    "print(\"Training complete. Model and results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e970e",
   "metadata": {},
   "source": [
    "## 5) Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a63198",
   "metadata": {},
   "source": [
    "### 5.1) Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3316eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(predictions, lat_min, lat_max, lon_min, lon_max):\n",
    "    \"\"\"\n",
    "    Decode normalized predictions back to original latitude and longitude.\n",
    "    Returns denormalized coordinates as a tensor.\n",
    "\n",
    "    Args:\n",
    "        predictions (Tensor): Normalized tensor of shape (..., 2).\n",
    "        lat_min (float): Minimum latitude used for normalization.\n",
    "        lat_max (float): Maximum latitude used for normalization.\n",
    "        lon_min (float): Minimum longitude used for normalization.\n",
    "        lon_max (float): Maximum longitude used for normalization.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Decoded predictions in original lat/lon coordinates.\n",
    "    \"\"\"\n",
    "    decoded = predictions.clone()\n",
    "    decoded[..., 0] = decoded[..., 0] * (lat_max - lat_min) + lat_min\n",
    "    decoded[..., 1] = decoded[..., 1] * (lon_max - lon_min) + lon_min\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e323cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_dist(coords1, coords2):\n",
    "    \"\"\"\n",
    "    Compute average Haversine distance (in km) between batches of coordinate pairs.\n",
    "    Inputs are iterables of (lat, lon) pairs.\n",
    "\n",
    "    Args:\n",
    "        coords1 (Iterable of tuples): First set of (lat, lon) coordinates.\n",
    "        coords2 (Iterable of tuples): Second set of (lat, lon) coordinates.\n",
    "\n",
    "    Returns:\n",
    "        float: Average Haversine distance across all pairs (in km).\n",
    "    \"\"\"\n",
    "    R = 6371.0  # Earth radius in kilometers\n",
    "    distance = 0.0\n",
    "\n",
    "    for coord1, coord2 in zip(coords1, coords2):\n",
    "        lat1, lon1 = coord1\n",
    "        lat2, lon2 = coord2\n",
    "\n",
    "        dlat = radians(lat2 - lat1)\n",
    "        dlon = radians(lon2 - lon1)\n",
    "\n",
    "        a = sin(dlat / 2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon / 2)**2\n",
    "        c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "        distance += R * c\n",
    "    return distance / len(coords1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "caa19f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADE(coords1, coords2):\n",
    "    \"\"\"\n",
    "    Compute Average Displacement Error (ADE) between two sequences of coordinates.\n",
    "    Inputs must be torch tensors in degrees.\n",
    "    Output is average error per time step in kilometers.\n",
    "\n",
    "    Args:\n",
    "        coords1 (Tensor): Predicted coordinates, shape (batch_size, seq_len, 2).\n",
    "        coords2 (Tensor): Ground truth coordinates, shape (batch_size, seq_len, 2).\n",
    "\n",
    "    Returns:\n",
    "        float: ADE in km.\n",
    "    \"\"\"\n",
    "    distance = 0.0\n",
    "    batch_size, seq_len, _ = coords1.shape\n",
    "    for i in range(seq_len):\n",
    "        # Compute average Haversine distance for each time step across the batch\n",
    "        distance += haversine_dist(coords1[:, i, :], coords2[:, i, :])\n",
    "    return distance / seq_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "56750377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FDE(coords1, coords2):\n",
    "    \"\"\"\n",
    "    Compute Final Displacement Error (FDE) between last predicted and true point.\n",
    "    Inputs must be torch tensors in degrees.\n",
    "    Output is error at last time step in kilometers.\n",
    "\n",
    "    Args:\n",
    "        coords1 (Tensor): Predicted coordinates, shape (batch_size, seq_len, 2).\n",
    "        coords2 (Tensor): Ground truth coordinates, shape (batch_size, seq_len, 2).\n",
    "\n",
    "    Returns:\n",
    "        float: FDE in km.\n",
    "    \"\"\"\n",
    "    final_pred = coords1[:, -1]\n",
    "    final_gt   = coords2[:, -1]\n",
    "\n",
    "    distance = 0.0\n",
    "    # Calculate Haversine distance for each pair in the batch\n",
    "    for pred_point, gt_point in zip(final_pred, final_gt):\n",
    "        distance += haversine_dist(pred_point.unsqueeze(0), gt_point.unsqueeze(0))\n",
    "    return distance / final_pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4fb0964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(coords1, coords2):\n",
    "    \"\"\"\n",
    "    Compute Root Mean Square Error (RMSE) between predicted and true coordinates.\n",
    "    Inputs must be torch tensors in degrees.\n",
    "    Output is root of average squared Haversine error in kilometers.\n",
    "\n",
    "    Args:\n",
    "        coords1 (Tensor): Predicted coordinates, shape (batch_size, seq_len, 2).\n",
    "        coords2 (Tensor): Ground truth coordinates, shape (batch_size, seq_len, 2).\n",
    "\n",
    "    Returns:\n",
    "        float: RMSE in km.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, _ = coords1.shape\n",
    "    mse = 0.0\n",
    "    for i in range(seq_len):\n",
    "        # Sum squared Haversine distances for each time step\n",
    "        mse += haversine_dist(coords1[:, i, :], coords2[:, i, :]) ** 2\n",
    "    mse /= seq_len\n",
    "    return sqrt(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1695066",
   "metadata": {},
   "source": [
    "### 5.2) Evaluate \n",
    "<br> In this notebook we trained the 1-day dataset, so we will evaluate this model as well. In the full project, we also evaluated the 4-days dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8551bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load normalization stats from training/validation to ensure consistent scaling.\n",
    "data_dir = \"datasplits\"\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'train', 'train_aisdk-2025-02-27.csv'))\n",
    "val_df = pd.read_csv(os.path.join(data_dir, 'val', 'val_aisdk-2025-02-27.csv'))\n",
    "lat_min = min(train_df['Latitude'].min(), val_df['Latitude'].min())\n",
    "lat_max = max(train_df['Latitude'].max(), val_df['Latitude'].max())\n",
    "lon_min = min(train_df['Longitude'].min(), val_df['Longitude'].min())\n",
    "lon_max = max(train_df['Longitude'].max(), val_df['Longitude'].max())\n",
    "sog_max = max(train_df['SOG'].max(), val_df['SOG'].max())\n",
    "    \n",
    "# Initialize LSTM and GRU models with same architecture as during training.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_gru = GRUModel(\n",
    "    input_size=5,\n",
    "    embed_size=64,\n",
    "    hidden_size=256,\n",
    "    output_size=2,\n",
    "    num_layers=2,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "model_lstm = LSTMModel(\n",
    "    input_size=5,\n",
    "    embed_size=64,\n",
    "    hidden_size=256,\n",
    "    output_size=2,\n",
    "    num_layers=2,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "model_gru.load_state_dict(torch.load('results/models/mae_final_model.pth', map_location=device))\n",
    "model_lstm.load_state_dict(torch.load('results/models/hav_final_model.pth', map_location=device))\n",
    "\n",
    "# Load test dataset using same normalization stats as training.\n",
    "testset = AISDataset(\n",
    "    os.path.join(data_dir, 'test', 'test_aisdk-2025-02-27.csv'),\n",
    "    seq_input_length=3,\n",
    "    seq_output_length=3,\n",
    "    stats=(lat_min, lat_max, lon_min, lon_max, sog_max)\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=32, shuffle=False, num_workers=1\n",
    ")\n",
    "\n",
    "# Evaluate both models using standard metrics (ADE, FDE, RMSE) on unnormalized outputs.\n",
    "model_lstm.eval()\n",
    "model_gru.eval()\n",
    "total_ade_lstm = 0\n",
    "total_fde_lstm = 0\n",
    "total_rmse_lstm = 0\n",
    "total_ade_gru = 0\n",
    "total_fde_gru = 0\n",
    "total_rmse_gru = 0\n",
    "\n",
    "print(\"Evaluating model...\")\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output_lstm = model_lstm(data)\n",
    "        output_gru = model_gru(data)\n",
    "        \n",
    "        # Unnormalize predicted and target coordinates for metric calculation.\n",
    "        output_lstm = decode_predictions(output_lstm, lat_min, lat_max, lon_min, lon_max)\n",
    "        output_gru = decode_predictions(output_gru, lat_min, lat_max, lon_min, lon_max)\n",
    "        target = decode_predictions(target, lat_min, lat_max, lon_min, lon_max)\n",
    "        # Sanity check\n",
    "        assert output_lstm.shape == target.shape, \\\n",
    "            f\"Shape mismatch: {output_lstm.shape} vs {target.shape}\"\n",
    "            \n",
    "        total_ade_lstm += ADE(output_lstm, target)\n",
    "        total_ade_gru += ADE(output_gru, target)\n",
    "        total_fde_lstm += FDE(output_lstm, target)\n",
    "        total_fde_gru += FDE(output_gru, target)\n",
    "        total_rmse_lstm += RMSE(output_lstm, target)\n",
    "        total_rmse_gru += RMSE(output_gru, target)\n",
    "\n",
    "average_ade_lstm = total_ade_lstm / len(test_loader)\n",
    "average_fde_lstm = total_fde_lstm / len(test_loader)\n",
    "average_rmse_lstm = total_rmse_lstm / len(test_loader)\n",
    "print(f\"Average ADE LSTM: {average_ade_lstm:.4f}\")\n",
    "print(f\"Average FDE LSTM: {average_fde_lstm:.4f}\")\n",
    "print(f\"Average RMSE LSTM: {average_rmse_lstm:.4f}\")\n",
    "\n",
    "average_ade_gru = total_ade_gru / len(test_loader)\n",
    "average_fde_gru = total_fde_gru / len(test_loader)\n",
    "average_rmse_gru = total_rmse_gru / len(test_loader)\n",
    "\n",
    "print(f\"Average ADE GRU: {average_ade_gru:.4f}\")\n",
    "print(f\"Average FDE GRU: {average_fde_gru:.4f}\")\n",
    "print(f\"Average RMSE GRU: {average_rmse_gru:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
